***

layout: post
title: "Knowledge Representation and Large Graph Models"
subtitle: ""
date: 2023-05-23
author: "Jamin Chen"
header-img: "img/post-bg-universe.jpg"
tags: \["Sapientia", "LGM", "Knowledge Representation"]
---------------------------

Summary: this article is a successive of our [project
Sapientia](http://xiaming.site/2023/03/12/project-sapientia/) for general
cognitive computing.

## Background

The core of a knowledge computing engine lies at an efficient and computational
representation of knowledge/logics. This requirement is also one of vital pieces
of future artificial general intelligence (AGI). Recent success of Transformer
in natural language field, especially with Large Language Models like GPT-4 and
LLaMA series, inspires us that neural networks (NNs) with Attention mechanism
are efficient at condensing information and logics encoded in sequential tokens
\[?]. However these models still remain as black box to uncover the inner
representation of learned knowledge. The main reason comes from the token
vectorization and high dimensional nature of neural networks with a vast amount
(tens to hundreds of billions) of parameters and deep layers.

We can imagine there is a higher dimensional space that absorbs and masters the
token occurrence patterns as vectors, also with their computational relations as
complex logics. Nevertheless we encounter a challenge to interpret, extract and
reuse those mastered patterns as explicit knowledge outside a trained model. The
representation of specific knowledge piece or logical relation remains varying
as for training context changes, neural network structures, and a bulk of
fine-tuning techniques. The models trained by different people, even a group of
models trained with the same datasets at different times are isolated and
unstable when considering their inner representations.

**Hypothesis-1**. LLM illustrates the capability to absorb world knowledge as
well as reasonable logics, because it masters not the world knowledge per se,
but the rules of knowledge organization in natural languages.

A portion of community endeavor is put into reversing engineering the black box
by prompt engineering \[?], single neuron interpretation \[?], network structural
analysis \[?] etc. We argue that the first principle thinking should be used to
derive the basic and core problem about what we are talking about knowledge
representation (KR). What is the most straight forward way to represent
knowledge? The answer is proposed as "symbolic". Human have gained the
capability to exchange information and relay history from generation to
generation as for the invention of symbolic expression, which also enhances our
logical reasoning under the help of symbolically (usually by native language)
driven inner voice \[?]. Hence the symbolic is still the native way of knowledge
representation.

## Limitations of LLM in KR

We have accumulated a large amount of knowledge in different forms, while the
symbolic conveys a significant portion. The symbolic is a convenient way to
communicate in speaking and writing. What LLM captures well is the patterns of
symbol sequences in which grammar and logics are encoded. Nevertheless there are
other forms of important symbolic knowledge, such as arithmetic equations, that
LLM can not deal well with. Many experiments have shown that LLM fails to
calculate addition and multiplication of arbitrary large numbers, much less
complicated mathematical theorem understanding. The core reason is that the
symbol 'plus' or '+' is not just a token in a sample sequence, but also with
calculating rules as predefined in math. A sequential regression model which
heavily relies on sample occurrences can not master this kind of implicit
knowledge well. In the natural language scenario, we are always surprised by the
knowledge scale and grammatical generation of LLM, while ignoring that there are
substantial related samples in the training datasets. Another explanation for
the weaknesses of LLM in arithmetic calculation is because we can not have a
training dataset contains a full enumeration of the symbolic calculation, except
for a specialized preparation.

**Hypothesis-2**. LLM presents superb capabilities in pattern capturing and
summarization, while showing an intrinsic weakness to involve the encoded
implicit knowledge in symbols.

## General Principles of KR

For general concept of knowledge, interpretation, reusability and integration
are the basic practicable demands in our daily life. In a similar sense,
knowledge representation in cognitive computing or AGI systems should also
possess these natures. Concretely,

* Interpretation, foundation stone for explainable knowledge systems. We could
    trace the source and reasoning process for specific conclusion. It is the
    vital part to construct paradigm of knowledge discovery, differing from
    knowledge squeezing and summarization, Hypothesis-1.

* Reusability, transferring summarized or discovered knowledge ingredients
    across different artificial intelligent systems. In LLM, we assume that
    complex relational dependencies between explicit knowledge ingredients are
    fetched, as some neural pathways created in creatural brains, being locked
    as black-boxed representations.

* Integration, exemplified as some kind of 'knowledge protocol' between
    systems. Almost every branch of our theories owns a suite of symbol
    framework to present its assumptions, axioms, theorems, deductions etc., for
    both precise communication and interaction with other frameworks.

## Knowledge Intermediate Representation

To address the challenges observed in LLMs, we suggest that knowledge
representation should be separated from communication and reasoning processes
and propose a concept of Knowledge Intermediate Representation (KIR) to fulfill
above principles. Some explicit and flexible structures, for example hypergraph,
are preferred for KIR. Recent progress in multimodal graph learning \[1] also
confirms the importance and powerfulness of selecting a suitable presentation
paradigm in a sense of fusing multimodal data sources.

![LLM vs KIR-LGM](/img/inpost/2023/KIR-LGM.png)

We compare the methodology of KIR with graphical reasoning such as large graph
model (LGM) or probabilistic graph reasoning (PGS) with current state-of-the-art
LLM technique, as illustrated in above figure. Here we consider the
interdisciplinary knowledge presented in symbols, such as natural languages,
system control instructions, physical theories and so on. We treat the input
human-readable symbols as a human-system interaction interface, defined as the
communication part. In left LLM way, all symbolic knowledge in training samples
are converted into token sequence and fed into the neural model where its
encoder-decoder structure with attention mechanism is efficient at parallel
training and capturing the long-term dependencies between tokens. Its
limitations are coupling of representation and reasoning, with challenges to
fulfill those KR principles depicted above.

In our proposed methodology, the general knowledge intermediate representation
part is introduced, as the right KIR way in above figure. It embodies the
principles of interpretation, reusability and integration for KR in a
declarative method. A Lisp-like language is recommended, as under the hook, each
list expression with a functor and variables could embody both the implicit and
explicit knowledge, which is also efficiently handled by other computational
techniques, such as symbolic regression [?]. Similar consideration is also made
in the project OpenCog Hyperon for MeTTa language [?]. KIR is the heart of our
framework, playing a role of 'inner voice' for a cognitive process and
memorizing ingredients in an artificial mind.

During the reasoning phase, given user's prompts, tokens are generated
recursively by the LLM fine-tuned targeting at specific task. This
sequence-in-sequence-out manner limits LLM's application in scenarios that the
information cannot be transformed into token sequence properly. For example,
when human looking at a video of a flock of birds hovering above the forest,
birds' flying patterns and landing location would be inferred and predicted with
our born sense of space and understanding of spatial relation of birds, sky and
forests. However, it is hard for LLM to formulate these spatial knowledge.
Although some showcases imply that LLM can answer space-concept-related physical
reasoning question, it is because that similar sequences describing related
problems are present in the training set and encoded in the hyper-parameter
space.

To conquer this limitation, we propose graph-modeled equivalent (e.g.
hypergraph) of KIR before performing reasoning. Also in the birds-hovering
example, we could embody the spatial relation of different objects in edges, as
well as physical knowledge encoded in edge-bundled functors. Ektefaie etc. [1]
employ a unified graph model to perform multimodal learning, which shares the
similar idea. In the reasoning procedure, the problem is formulated as partial
substitution of masked subgraphs, in other words, performing subgraph matching
given a variable-embedded graph (variable grounding). We categorize techniques
into two groups, searching paradigm and graph neural network (GNN) paradigm.
Both could meet our requirements, but have differences in the scale of searching
space and computing efficiency.

## Why Large Graph Model?

*   Large Graph Model

## Future

## References

\[1] Ektefaie, Y., Dasoulas, G., Noori, A. et al. Multimodal learning with
graphs. Nat Mach Intell 5, 340â€“350 (2023).
https://doi.org/10.1038/s42256-023-00624-6
