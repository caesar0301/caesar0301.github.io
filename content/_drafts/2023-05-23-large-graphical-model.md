---
layout: post
title: "Large Graphical Model to Condense Knowledge and Logics"
subtitle: ""
date: 2023-05-23
author: "Jamin Chen"
header-img: "img/post-bg-universe.jpg"
tags: ["Sapientia", "LLM"]
---

## Background

The core of a knowledge computing engine lies at the efficient and computational
representation of knowledge/logics. This requirement is also one of vital pieces
of future artificial general intelligence (AGI). Recent success of Transformer
technique in natural language field, especially with Large Language Models like
GPT-4 and LLaMA series, inspires us *the efficiency of neural networks with
Attention mechanism in condense the information, even logics, encoded in
sequential tokens* [?]. However these models still remain as black box to
uncover the inner representation of learned knowledge/logics. The main reason
comes from the token vectorization and high dimensional nature of neural
networks with a vast amount (tens to hundreds of billions) of parameters and
deep layers. We can imagine that knowledge/logics are encoded in higher
dimensional space while these higher-spaced vectors, as well as their
computational relations, still remain isolated and unstable across different
models. 

A portion of community endeavor is struggling in reverse engineering the black
box by prompt engineering [?], single neuron semantics interpretation [?],
theoretical mirroring structural analysis [?] etc. We argue that a rethink from
the starting point of original problem is required. The original problem: what
is the most proper and promising representation of knowledge/logics? Then the
following question after that is how to represent it efficiently.

## Knowledge Representation
* Disadvantages of knowledge in NL
* Hypergraph enabled
* Natural integration of cognitive and actionable knowledge?

## Generative Graphical Patterns
* Large Graphical Models

## A General Framework towards LGM
NL -> klang (hypergraph enhanced) -> LGM -> klang -> NL

## How To Engineer LGM

## Future

